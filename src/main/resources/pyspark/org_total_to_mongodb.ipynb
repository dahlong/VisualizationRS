{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========read csv data============ 2016-05-12 03:38:32.237695\n",
      "============group by org_id / month data============\n",
      "============preparing list for mongodb============\n",
      "============start insert organzation statistics into mongodb============\n",
      "============finish to insert org total into mongodb============ 2016-05-12 03:39:42.232023\n"
     ]
    }
   ],
   "source": [
    "#dataframes, org with total and month\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pymongo import MongoClient\n",
    "from collections import OrderedDict\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "\n",
    "client = MongoClient(\"mongodb://192.168.187.129:27017\")\n",
    "db = client.hrvisual\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "strYear='2015'\n",
    "\n",
    "collectionName = 'LEAVE_TYPE_%s' % strYear\n",
    "#if the collection exises in mongodb, then drop it\n",
    "#if collName in db.collection_names():\n",
    "#        db[collName].drop()            \n",
    "\n",
    "def getRddWithoutHeader(csvfname, fieldname):\n",
    "    f  = sc.textFile(csvfname)\n",
    "    r = f.filter(lambda l: fieldname in l)\n",
    "    r.collect()\n",
    "    r1 = f.subtract(r)\n",
    "    d=r1.map(lambda k: k.split(\",\"))\n",
    "    return d\n",
    "\n",
    "arrayferialname=[\"特別休假\",\"加班或假日出差轉補休\",\"生理假\",\"傷病假\",\"婚假\",\"家庭照顧假\", \\\n",
    "                    \"事假\",\"產檢假\",\"陪產假\",\"產假\",\"喪假\",\"國內公假\",\"國外公假\",\"公傷病假\", \"安胎假\"]\n",
    "\n",
    "def getFerialDict(fList, start):\n",
    "    mydic={}\n",
    "    for i in range(0,len(arrayferialname),1):\n",
    "        mydic[arrayferialname[i]]=int(fList[i+start])\n",
    "    \n",
    "    return mydic\n",
    "\n",
    "\n",
    "#build up schema\n",
    "structFields=[StructField(\"org_id\", StringType()), StructField(\"org_name\", StringType()), StructField(\"month\", IntegerType())]\n",
    "for i in range(1,len(arrayferialname) +1,1):\n",
    "    structFields.append(StructField(arrayferialname[i-1], IntegerType()))\n",
    "\n",
    "schema = StructType(structFields)\n",
    "\n",
    "print \"===========read csv data============\", datetime.datetime.now()\n",
    "data=getRddWithoutHeader('LEAVE_TYPE_%s.csv' % strYear,'org_id')\n",
    "lines_temp = data.map(lambda p: (str(p[0]),unicode(p[1]),int(p[10]),int(p[11]),int(p[12]),int(p[13]),int(p[14]),int(p[15]), \\\n",
    "            int(p[16]),int(p[17]),int(p[18]),int(p[19]),int(p[20]),int(p[21]),int(p[22]),int(p[23]),int(p[24]),int(p[25])))\n",
    "\n",
    "lines_df = sqlContext.createDataFrame(lines_temp, schema)\n",
    "#print lines_df.printSchema()\n",
    "\n",
    "print \"============group by org_id / month data============\"\n",
    "line_group=lines_df.groupBy(\"org_id\", \"org_name\").sum('特別休假','加班或假日出差轉補休','生理假','傷病假','婚假','家庭照顧假', \\\n",
    "            '事假','產檢假', '陪產假','產假','喪假','國內公假','國外公假','公傷病假', '安胎假')\n",
    "\n",
    "line_group_month=lines_df.groupBy(\"org_id\", \"org_name\", \"month\").sum('特別休假','加班或假日出差轉補休','生理假','傷病假','婚假','家庭照顧假', \\\n",
    "            '事假','產檢假', '陪產假','產假','喪假','國內公假','國外公假','公傷病假', '安胎假')\n",
    "\n",
    "line_sort=line_group.sort(asc(\"org_id\")).collect()\n",
    "print \"============preparing list for mongodb============\"\n",
    "mylist = []\n",
    "for a in line_sort: \n",
    "    rowDb={}\n",
    "    rowDb[\"org_id\"]=a[0]\n",
    "    rowDb[\"org_name\"]=a[1]\n",
    "    rowDb[\"Total\"]=getFerialDict(a, 2)\n",
    "    line_filter=line_group_month.filter(line_group_month.org_id==a[0]).collect()\n",
    "    for b in line_filter:\n",
    "        rowDb[str(b[2])]=getFerialDict(b, 3)\n",
    "        \n",
    "    #print rowDb   \n",
    "    rowDb=OrderedDict(sorted(rowDb.items(), key=lambda t: t[0]))\n",
    "    mylist.append(rowDb)\n",
    "    #result=db[collectionName].insert_one(rowDb)    \n",
    "\n",
    "print \"============start insert organzation statistics into mongodb============\"\n",
    "result=db[collectionName].insert_many(mylist)       \n",
    "print \"============finish to insert org total into mongodb============\", datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish to insert into mongodb\n"
     ]
    }
   ],
   "source": [
    "#dataframes including month\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pymongo import MongoClient\n",
    "from collections import OrderedDict\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "client = MongoClient(\"mongodb://192.168.187.129:27017\")\n",
    "db = client.hrvisual\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "strYear='2015'\n",
    "\n",
    "collectionName = 'LEAVE_TYPE_TEST_%s' % strYear\n",
    "#if the collection exises in mongodb, then drop it\n",
    "#if collName in db.collection_names():\n",
    "#        db[collName].drop()            \n",
    "\n",
    "def getRddWithoutHeader(csvfname, fieldname):\n",
    "    f  = sc.textFile(csvfname)\n",
    "    r = f.filter(lambda l: fieldname in l)\n",
    "    r.collect()\n",
    "    r1 = f.subtract(r)\n",
    "    d=r1.map(lambda k: k.split(\",\"))\n",
    "    return d\n",
    "\n",
    "arrayferialname=[\"特別休假\",\"加班或假日出差轉補休\",\"生理假\",\"傷病假\",\"婚假\",\"家庭照顧假\", \\\n",
    "                    \"事假\",\"產檢假\",\"陪產假\",\"產假\",\"喪假\",\"國內公假\",\"國外公假\",\"公傷病假\", \"安胎假\"]\n",
    "\n",
    "#build up schema\n",
    "structFields=[StructField(\"org_id\", StringType()), StructField(\"org_name\", StringType()), StructField(\"month\", IntegerType())]\n",
    "for i in range(1,len(arrayferialname) +1,1):\n",
    "    structFields.append(StructField(arrayferialname[i-1], IntegerType()))\n",
    "\n",
    "schema = StructType(structFields)\n",
    "\n",
    "\n",
    "data=getRddWithoutHeader('LEAVE_TYPE_%s.csv' % strYear,'org_id')\n",
    "lines_temp = data.map(lambda p: (str(p[0]),unicode(p[1]),int(p[10]),int(p[11]),int(p[12]),int(p[13]),int(p[14]),int(p[15]),int(p[16]), \\\n",
    "    int(p[17]),int(p[18]),int(p[19]),int(p[20]),int(p[21]),int(p[22]),int(p[23]),int(p[24]),int(p[25])))\n",
    "\n",
    "lines_df = sqlContext.createDataFrame(lines_temp, schema)\n",
    "#print lines_df.printSchema()\n",
    "\n",
    "line_group=lines_df.groupBy(\"org_id\", \"org_name\", \"month\").sum('特別休假','加班或假日出差轉補休','生理假','傷病假','婚假','家庭照顧假', \\\n",
    "            '事假','產檢假', '陪產假','產假','喪假','國內公假','國外公假','公傷病假', '安胎假')\n",
    "\n",
    "#insert into mongodb\n",
    "line_sort=line_group.sort(asc(\"org_id\")).collect()\n",
    "for a in line_sort: \n",
    "    rowDb={}\n",
    "    rowDb[\"org_id\"]=a[0]\n",
    "    rowDb[\"org_name\"]=a[1]\n",
    "    rowDb[\"month\"]=a[2]\n",
    "    \n",
    "    sumArray={}\n",
    "    for i in range(0,len(arrayferialname),1):\n",
    "        sumArray[arrayferialname[i]]=int(a[i+3])\n",
    "    \n",
    "    rowDb[\"Total\"]=sumArray\n",
    "    rowDb=OrderedDict(sorted(rowDb.items(), key=lambda t: t[0]))\n",
    "    #print 'rowDb=', rowDb\n",
    "    result=db[collectionName].insert_one(rowDb)\n",
    "    \n",
    "\n",
    "print \"finish to insert into mongodb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish to insert into mongodb\n"
     ]
    }
   ],
   "source": [
    "#dataframes\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pymongo import MongoClient\n",
    "from collections import OrderedDict\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "client = MongoClient(\"mongodb://192.168.187.129:27017\")\n",
    "db = client.hrvisual\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "strYear='2015'\n",
    "\n",
    "collectionName = 'LEAVE_TYPE_%s' % strYear\n",
    "#if the collection exises in mongodb, then drop it\n",
    "#if collName in db.collection_names():\n",
    "#        db[collName].drop()            \n",
    "\n",
    "def getRddWithoutHeader(csvfname, fieldname):\n",
    "    f  = sc.textFile(csvfname)\n",
    "    r = f.filter(lambda l: fieldname in l)\n",
    "    r.collect()\n",
    "    r1 = f.subtract(r)\n",
    "    d=r1.map(lambda k: k.split(\",\"))\n",
    "    return d\n",
    "\n",
    "arrayferialname=[\"特別休假\",\"加班或假日出差轉補休\",\"生理假\",\"傷病假\",\"婚假\",\"家庭照顧假\", \\\n",
    "                    \"事假\",\"產檢假\",\"陪產假\",\"產假\",\"喪假\",\"國內公假\",\"國外公假\",\"公傷病假\", \"安胎假\"]\n",
    "\n",
    "#build up schema\n",
    "structFields=[StructField(\"org_id\", StringType()), StructField(\"org_name\", StringType())]\n",
    "for i in range(1,len(arrayferialname) +1,1):\n",
    "    structFields.append(StructField(arrayferialname[i-1], IntegerType()))\n",
    "\n",
    "schema = StructType(structFields)\n",
    "\n",
    "\n",
    "data=getRddWithoutHeader('LEAVE_TYPE_%s.csv' % strYear,'org_id')\n",
    "lines_temp = data.map(lambda p: (str(p[0]),unicode(p[1]),int(p[11]),int(p[12]),int(p[13]),int(p[14]),int(p[15]),int(p[16]), \\\n",
    "    int(p[17]),int(p[18]),int(p[19]),int(p[20]),int(p[21]),int(p[22]),int(p[23]),int(p[24]),int(p[25])))\n",
    "\n",
    "lines_df = sqlContext.createDataFrame(lines_temp, schema)\n",
    "#print lines_df.printSchema()\n",
    "\n",
    "line_group=lines_df.groupBy(\"org_id\", \"org_name\").sum('特別休假','加班或假日出差轉補休','生理假','傷病假','婚假','家庭照顧假', \\\n",
    "            '事假','產檢假', '陪產假','產假','喪假','國內公假','國外公假','公傷病假', '安胎假')\n",
    "\n",
    "#insert into mongodb\n",
    "line_sort=line_group.sort(asc(\"org_id\")).collect()\n",
    "for a in line_sort: \n",
    "    rowDb={}\n",
    "    rowDb[\"org_id\"]=a[0]\n",
    "    rowDb[\"org_name\"]=a[1]\n",
    "    \n",
    "    sumArray={}\n",
    "    for i in range(0,len(arrayferialname),1):\n",
    "        sumArray[arrayferialname[i]]=int(a[i+2])\n",
    "    \n",
    "    rowDb[\"Total\"]=sumArray\n",
    "    rowDb=OrderedDict(sorted(rowDb.items(), key=lambda t: t[0]))\n",
    "    #print 'rowDb=', rowDb\n",
    "    result=db[collectionName].insert_one(rowDb)\n",
    "    \n",
    "'''\n",
    "lines_df.registerTempTable(\"tempTable\")\n",
    "\n",
    "sqlcon=sqlContext.sql(\"SELECT org_id, org_name, SUM(特別休假) as 特別休假, SUM(加班或假日出差轉補休) as 加班或假日出差轉補休, \\\n",
    "        SUM(生理假) as 生理假,SUM(傷病假) as 傷病假,SUM(婚假) as 婚假,SUM(家庭照顧假) as 家庭照顧假,SUM(事假) as 事假, \\\n",
    "        SUM(產檢假) as 產檢假,SUM(陪產假) as 陪產假,SUM(產假) as 產假,SUM(喪假) as 喪假,SUM(國內公假) as 國內公假, \\\n",
    "        SUM(國外公假) as 國外公假,SUM(公傷病假) as 公傷病假,SUM(安胎假) as 安胎假 FROM tempTable GROUP BY org_id, org_name\")\n",
    "\n",
    "#print sqlcon\n",
    "print sqlcon.show()\n",
    "'''\n",
    "print \"finish to insert into mongodb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#以下統計各中心的各假別total\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from operator import add\n",
    "from pymongo import MongoClient\n",
    "from collections import OrderedDict\n",
    "\n",
    "client = MongoClient(\"mongodb://192.168.187.129:27017\")\n",
    "db = client.hrvisual\n",
    "\n",
    "strYear='2015'\n",
    "\n",
    "collName = 'LEAVE_TYPE_%s' % strYear\n",
    "\n",
    "dataFileName='LEAVE_TYPE_%s.csv' % strYear\n",
    "\n",
    "def getRddWithoutHeader(csvfname, fieldname):\n",
    "    f  = sc.textFile(csvfname)\n",
    "    r = f.filter(lambda l: fieldname in l)\n",
    "    r.collect()\n",
    "    r1 = f.subtract(r)\n",
    "    d=r1.map(lambda k: k.split(\",\"))\n",
    "    return d\n",
    "\n",
    "\n",
    "def filter_org(s,pos, v):\n",
    "    if s[pos] == v:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def getOrgIdList(orgFile):\n",
    "    org_data=getRddWithoutHeader(orgFile,'org_id')\n",
    "    #org_1 = org_parts.map(lambda p: (str(p[0]),  p[1].encode('utf-8')))\n",
    "    o1 = org_data.map(lambda p: (str(p[0]),  unicode(p[1])))\n",
    "    o2=sc.parallelize(o1.top(o1.count()))    \n",
    "    orglist=sorted(o2.collect())\n",
    "    arrId=[]\n",
    "    arrName=[]\n",
    "    for o in  range(0,o2.count(),1):\n",
    "        arrId.append(orglist[o][0])\n",
    "        arrName.append(orglist[o][1])\n",
    "        \n",
    "    return arrId, arrName\n",
    "    \n",
    "    \n",
    "def getFerialArray(fName):\n",
    "    f1=sc.textFile(fName)\n",
    "    f2 = f1.filter(lambda l: 'org_id' in l)\n",
    "    f3=f2.map(lambda k: k.split(\",\"))\n",
    "    arrf=[]\n",
    "    for i in range (7, 22,1):\n",
    "        arrf.append(f3.top(1)[0][i])\n",
    "    \n",
    "    return arrf\n",
    "\n",
    "\n",
    "#特別休假,加班或假日出差轉補休,生理假,傷病假,婚假,家庭照顧假,事假,產檢假,陪產假,產假,喪假,國內公假,國外公假,公傷病假,安胎假\n",
    "arrayferialname= getFerialArray(dataFileName)\n",
    "#print arrayferialname\n",
    "\n",
    "data=getRddWithoutHeader(dataFileName,'org_id')\n",
    "\n",
    "#取組織 org_id, org_nmame\n",
    "orgIdArray, orgNameArray=getOrgIdList(orgFileName)\n",
    "narlOrg = dict(zip(orgIdArray, orgNameArray))\n",
    "#print narlOrg\n",
    "\n",
    "for org_id, org_name in narlOrg.iteritems() :\n",
    "    total_org = data.map(lambda p: (str(p[0]),int(p[7]), int(p[8]),int(p[9]),int(p[10]),int(p[11]),int(p[12]), \\\n",
    "                                     int(p[13]),int(p[14]), int(p[15]),int(p[16]),int(p[17]),int(p[18]),int(p[19]), \\\n",
    "                                     int(p[20]),int(p[21])))\n",
    "    rdd1 = sc.parallelize(total_org.top(total_org.count()))\n",
    "    f_org_1 = rdd1.filter(lambda s: filter_org(s,0,org_id))\n",
    "    print 'org_id=',org_id, \" 共幾筆記錄:\", f_org_1.count()\n",
    "    rowDb={}\n",
    "    rowDb[\"org_id\"]=org_id\n",
    "    rowDb[\"org_name\"]=org_name\n",
    "    #print 'rowDb=', rowDb\n",
    "    \n",
    "    sumArray={}\n",
    "    for vi in range(1,len(arrayferialname) +1,1):\n",
    "        if (f_org_1.count()>0):\n",
    "            f_org_2=f_org_1.map(lambda p: (str(p[0]),int(p[vi])))\n",
    "            rdd3 = sc.parallelize(f_org_2.top(f_org_2.count()))\n",
    "            sum_1=sorted(rdd3.reduceByKey(add).collect())\n",
    "            sumArray[arrayferialname[vi-1]]=sum_1[0][1]\n",
    "        else:\n",
    "            sumArray[arrayferialname[vi-1]]=int(0)\n",
    "        \n",
    "        \n",
    "    rowDb[\"Total\"]=sumArray\n",
    "    rowDb=OrderedDict(sorted(rowDb.items(), key=lambda t: t[0]))\n",
    "    #print 'rowDb=',rowDb\n",
    "    \n",
    "    #if the collection exises in mongodb, then drop it\n",
    "    #if collName in db.collection_names():\n",
    "    #        db[collName].drop()            \n",
    "    \n",
    "    #print 'start to inser into mongodb'        \n",
    "    if len([rowDb])>0:\n",
    "        result=db[collName].insert_many([rowDb])\n",
    "               \n",
    "\n",
    "\n",
    "print '%s 年度，各中心寫入 mongodb 結束.....' %strYear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
